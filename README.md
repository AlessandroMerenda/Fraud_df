Questo progetto nasce dall’esigenza di esplorare come affrontare e gestire dataset complessi, preparando i dati per analisi più approfondite. Non mi sono posto come obiettivo principale la previsione effettiva di frodi, ma piuttosto la creazione di un processo replicabile e versatile che possa essere adattato a contesti diversi.

Cosa ho fatto:

    - Utilizzo di tecniche di bilanciamento come SMOTE per   migliorare la qualità dei dati.
    - Implementazione e ottimizzazione modelli di machine learning come Random Forest e XGBoost, con un’attenzione particolare all’ottimizzazione dei parametri (Bayesian Optimization).
    - Focus su un approccio strutturato e chiaro, che possa essere facilmente replicato.

Obiettivo principale: Dimostrare come affrontare dataset complessi e prepararli per analisi più approfondite, con un processo che possa essere adattato ad altri problemi.

Stato attuale: Il progetto è in corso di sviluppo e continuerà a evolversi nei prossimi giorni.